{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU5fTbDqCBvY"
      },
      "outputs": [],
      "source": [
        "--> lets revise the ml and deep\n",
        "what is the ml\n",
        "the ml is the subpart of the artificial intellegience  in which the machine are learn form the data by its self without explicitly coding by the programmer\n",
        "the ml is again devide into the two parts supervised amchine learning 2. unsupervised machine learning\n",
        "in the superived machine learning the data has the label the label means we know both independent varible and the dependent varible\n",
        "means --> suppose we have a dataset about the spam so in the case we have the meessege and the label the perticular  messgae is spam or not so here we have given the both the label and the predicted value\n",
        "in case of theunsupervised machine learning we dont have the label so the data is unlabel so we have to find the feature inside the data means we have to goroup that data by there features like clustring\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "so the next step is the feature engineering\n",
        "in the feature engineering we do the some basic task like the feture selection feature construction feture extraction\n",
        "so in the feature engineering we perfrom the EDA the eda stand for the exploretory data anlysis\n",
        "lets start-->\n",
        "so for perform some basic EDA\n",
        "-->check the coluns df.columns()\n",
        "-->check the row df.head() , df.tail()\n",
        "--> check the shape --> df.shape\n",
        "-->check the info -->df.info()\n",
        "-->discribe the data-->df.describe()\n",
        "-->drop the columns-->df.drop(columns=[])\n",
        "-->for check the null val-->df.isnull().sum()\n",
        "-->for check is the value in that perticular column-->df.isin()\n",
        "-->to fill null value with some othere perticular values  -->df.fillna()\n",
        "-->to drop the null column-->df.dropna\n",
        "-->to check the index name-->df.index\n",
        "-->for apply some fuction on the data-->df.apply(function_name)\n",
        "-->for deviding the data-->df.iloc\n",
        "-->for check the perticular condiction-->df[[val>0]]\n",
        "-->for mergeing the two dataset-->df=df[dataset one]+data[dataset2]"
      ],
      "metadata": {
        "id": "ULF-r3iGFzpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-->ploting the some graph by using the seaborn\n",
        "sns.pairplot()\n",
        "sns.lmplot(df , x=\" \" ,y=\" \" , hue=\" \")\n",
        "sns.scatterplot(df , x=\" \" ,y=\" \" , hue=\" \")-->reletional plot\n",
        "sns.displot(df , x=\" \" ,y=\" \" , hue=\" \")-->relectional plot\n",
        "sns.catplot(df , x=\" \" ,y=\" \" , hue=\" \")-->catogoraical plot\n",
        "sns.lineplot(df , x=\" \" ,y=\" \" , hue=\" \")-->relectional plot\n",
        "sns.histplot(df , x=\" \" ,y=\" \" , hue=\" \")-->distribution plot\n",
        "sns.barplot(df , x=\" \" ,y=\" \" , hue=\" \")-->catogorical plot\n",
        "sns.kdeplot(df , x=\" \" ,y=\" \" , hue=\" \")-->distribution plot\n",
        "sns.countplot(df , x=\" \" ,y=\" \" , hue=\" \")-->catogorical plot\n",
        "sns.striplot(df , x=\" \" ,y=\" \" , hue=\" \")-->a scatrer for catogorical plot\n",
        "sns.swarmplot(df , x=\" \" ,y=\" \" , hue=\" \")-->catogorical plot\n",
        "sns.violinplot(df , x=\" \" ,y=\" \" , hue=\" \")-->catogorical plot\n",
        "sns.pointplot(df , x=\" \" ,y=\" \" , hue=\" \")-->catogorical plot\n",
        "sns.ecdfplot(df , x=\" \" ,y=\" \" , hue=\" \")-->catogorical plot\n",
        "sns.histmap()-->matrix map\n",
        "sns.clustermap()-->matrics map\n",
        "sns.facetgrid()-->othere plot\n",
        "sns.treemap()-->othere plot\n",
        "sns.jointplot()-->grid plot\n",
        "sns.regplot(df , x=\" \" ,y=\" \" , hue=\" \")-->regression plot\n",
        "sns.residplot(df , x=\" \" ,y=\" \" , hue=\" \")-->regression plot\n",
        "sns.catplot()-->grid plot"
      ],
      "metadata": {
        "id": "lFw_XtqpH-Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "so now the eda is complete after this the next step is to feature  engineering the perticular step used the the feature enginerring\n",
        "\n",
        "the use of the skitlearn library\n",
        "-->fill the perticualr value -->imputer\n",
        "-->diffrent type of the imputers\n",
        "-->spliting the data train and text\n",
        "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#simple imputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer=SimpleImputer(missing_values=np.nan , strategy='mean')\n",
        "X=imputer.fit_transform(X)\n",
        "\n",
        "#median imputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer=SimpleImputer(missing_values=np.nan , strategy='median')\n",
        "X=imputer.fit_transform(X)\n",
        "\n",
        "#mode imputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer=SimpleImputer(missing_values=np.nan , strategy='most_frequently')\n",
        "X=imputer.fit_transform(X)\n",
        "\n",
        "#knn imputer\n",
        "from the sklearn.impute import KNNImputer\n",
        "imputer=KNNImputer(n_neighbour=2)\n",
        "X=fit_transform(X)\n",
        "\n",
        "#itrative imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "imputer=IterativeImputer(missing_values=np.nan , strategy='mean')\n",
        "X=imputer.fit_transform(X)\n",
        "\n",
        "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#splittin the data\n",
        "-->using the train test split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train , y_train , X_test , y_test=train_test_split(X ,y , test_size=0.2 , random_state=0)\n",
        "\n",
        "-->when the data is imbalence\n",
        "how we can handle the imbalnce data the imbalce is the condiction where the data distribution is not equal means if the binart classification problem  there if\n",
        "there is 95% is the negaive and %5 positive the the problem is model go to precision and the recall\n",
        "\n",
        "how to handle the imbalnce\n",
        "-->under sampling--> it involve reducing the majority column in the data to make the balance\n",
        "-->oversampling-->it invole increse the sample artifical by creting the duplicate instant of the minority data\n",
        "\n",
        "-->in the oversampling tequenic we used the the smote trquenic\n",
        " -->synthetic minority over-sampling tequenic\n",
        "\n",
        "\n",
        " -->before doing the smote we do train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train , y_train , X_test , y_test=train_test_split(X ,y , test_size=0.2 , random_state=0)\n",
        "\n",
        "code -->oversampling\n",
        " from imbrean.oversampling import SMOTE\n",
        "sm=SMOTE(random_state=42)\n",
        "sm.fit_resample(X_train , y_train,)\n",
        "\n",
        "Imagine a scenario where you have a dataset of email spam classification:\n",
        "\n",
        "Class 0 (Not Spam): 99% of emails\n",
        "Class 1 (Spam): 1% of emails\n",
        "\n",
        "so  to make the data balance used the emsemble learning like the random forest\n",
        "\n",
        "after the dpliting of the data the\n",
        "-->scalin the data\n",
        "convert the high feature data into the low range\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from skearn.preprocessing import RobustScaler\n",
        "from skalern.preprocessing import MaxabScaler\n",
        "\n",
        "\n",
        "sc=StandardScaler()\n",
        "sc=MinMaxScaler()\n",
        "sc=RobustScaler()\n",
        "sc=MaxabScaler()\n",
        "\n",
        "sc.fit_transform(X_train)\n",
        "sc.fit(X_test)\n",
        "\n",
        "\n",
        "# to make the data normal disribution\n",
        "# power transformer\n",
        "#log transformer\n",
        "\n",
        "-->when the data is right skewd the in that case we used the log transformer\n",
        "-->box-cox and the yeo-jonson\n",
        "\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "pt = PowerTransformer(method='yeo-johnson')\n",
        "X_yeo = pt.fit_transform(X.reshape(-1, 1))\n",
        "\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "pt = PowerTransformer(method='box-cox')\n",
        "X_yeo = pt.fit_transform(X.reshape(-1, 1))\n",
        "\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "trans=QuantileTrasformer()\n",
        "\n",
        "after the  converting this all the info the next step the conerting the catogorical feature in the numerical feature beacause as we know we can not work with the catogorical data\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "ct=ColumnTransformer(transformers=['encoder' , OneHotEncoder() ,[1]] , inplace=True)\n",
        "X_train=ct.fit_transform(X_train)\n",
        "\n",
        "le=LabelEncoder()\n",
        "or=OrdinalEncoder(categories=['high ' , 'low'])\n",
        "fit_transfrom(X)\n",
        "\n",
        "numerical_features = ['numerical_column1', 'numerical_column2']\n",
        "categorical_features = ['categorical_column1']\n",
        "\n",
        "# Create a preprocessing pipeline for numerical data\n",
        "num_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Create a preprocessing pipeline for categorical data\n",
        "cat_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(sparse=False, drop='first'))\n",
        "])\n",
        "\n",
        "# Combine numerical and categorical pipelines into one column transformer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', num_pipeline, numerical_features),\n",
        "    ('cat', cat_pipeline, categorical_features)\n",
        "])\n",
        "\n",
        "# Apply the preprocessing to the training and test data\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "#load the data\n",
        "data=pd.read_csv(data)\n",
        "X=data.iloc[: ,-1].values\n",
        "y=data.iloc[: ,].values\n",
        "\n",
        "# Calculate Q1 (25th percentile) and Q3 (75th percentile) for each column\n",
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Identify outliers by checking if data is outside of (Q1 - 1.5 * IQR) or (Q3 + 1.5 * IQR)\n",
        "outliers = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR)))\n",
        "\n",
        "# Print outliers\n",
        "print(\"Outliers detected:\")\n",
        "print(df[outliers.any(axis=1)])\n",
        "\n",
        "# Remove rows where data is outside the IQR bounds\n",
        "df_no_outliers = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "print(\"Data after removing outliers:\")\n",
        "print(df_no_outliers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d5OL79XLMWwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "after this the next step is the it is the simple step in the creation preocessing\n",
        "\n",
        "regression-->the regreesion is the supervised ml tequnic in which we will predict the point we do the best fit line\n",
        "classification-->it is the also the supervised ml tequenic in which we used classifi the classes\n",
        "\n",
        "all the classification and regression types\n",
        "\n",
        "simple regression-->when in the dataset one independent and one dependent  used the simple regression\n",
        "\n",
        "from sklearn.linear_model"
      ],
      "metadata": {
        "id": "6Ppxh0M_WvOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "now we are in the regression and the classification\n",
        "so in the linear regression it is the one of the simple regression type in that actually we have given data is must be linear first asummption second is the data is linearly sepreble third is the data do not contain the outlier do not have non-linearity\n",
        "so we can apply the linear regression on that   the linear regression is the type of the regression in which we have one dependent var and one independent var\n",
        "so after the linear regression the next is the logestic regression the logestic regression in which we have a classes means its highly used for the classification\n",
        "it give answer either true or false os the yes or no so it is consider one thresold in which the value is eithere true or false\n",
        "after the logestic the next is the svm  svr and svc\n",
        "in the svm we have given the we have\n",
        "\n",
        "\n",
        "some regression matrics evalution\n",
        "Mean Squared Error (MSE): The average of the squares of the errors (difference between predicted and actual values).\n",
        "Mean Absolute Error (MAE): The average of the absolute differences between predicted and actual values.\n",
        "R-squared (R¬≤): A measure of how well the regression model fits the data (higher values indicate a better fit).\n",
        ""
      ],
      "metadata": {
        "id": "e5iwzKYOVaqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1. Simple Linear Regression (Basic Concept)\n",
        "Goal: Find a line that best fits the data points.\n",
        "\n",
        "Concept: The basic idea is to fit a straight line (in 2D space) or a hyperplane (in multi-dimensional space) through the data that minimizes the distance between the line and the data points (errors).\n",
        "\n",
        "Intuition: You're essentially trying to predict a target value by drawing a straight line (or flat surface) through the data that balances the error (how far the actual points are from the predicted values).\n",
        "\n",
        "Key Insight: The algorithm tries to find the line that best represents the relationship between the features and the target by minimizing squared errors (difference between actual and predicted values).\n",
        "\n",
        "2. Polynomial Regression (Extension of Linear)\n",
        "Goal: Fit a curved line or surface that captures non-linear relationships between the input features and the target.\n",
        "\n",
        "Concept: If the relationship between features and target isn't a straight line, polynomial regression fits a curve (or higher-order polynomial function) to better capture the complexity of the data.\n",
        "\n",
        "Intuition: Instead of a simple straight line, you're using curves that can bend to match the data more closely. This allows the model to capture non-linear patterns.\n",
        "\n",
        "\n",
        "\n",
        "Where the degree\n",
        "ùëõ\n",
        "n of the polynomial is adjusted to capture more complexity.\n",
        "Key Insight: The model adds higher powers of the input features (like\n",
        "ùëãX\n",
        "3\n",
        " ) to capture more complex relationships between the features and target.\n",
        "\n",
        "3. Ridge and Lasso Regression (Regularization)\n",
        "Goal: Prevent overfitting by adding a penalty for large coefficients in the model.\n",
        "\n",
        "Concept: Both Ridge and Lasso are variations of linear regression that add a penalty term to the model to prevent it from becoming too complex and overfitting the data. Overfitting happens when the model fits too closely to the training data, including noise, leading to poor performance on new data.\n",
        "\n",
        "Ridge (L2 Regularization): Adds a penalty proportional to the square of the coefficients.\n",
        "Lasso (L1 Regularization): Adds a penalty proportional to the absolute value of the coefficients, which encourages sparsity (some features may be entirely ignored).\n",
        "Intuition: Imagine you‚Äôre fitting a line to data, but you want to keep the model simple and avoid letting it become too influenced by outliers or noise. Adding a penalty forces the model to choose smaller weights for the features, leading to a simpler, more general model.\n",
        "\n",
        "Key Insight: Regularization helps to smooth out the model and improve its ability to generalize to unseen data.\n",
        "\n",
        "4. Decision Tree Regression (Non-linear Model)\n",
        "Goal: Partition the feature space into regions and predict the average of the target variable in each region.\n",
        "\n",
        "Concept: Decision trees divide the data into distinct branches based on different features, leading to a tree-like structure. In each leaf of the tree, the predicted value is the average of the target variable for the instances in that leaf.\n",
        "\n",
        "Intuition: Instead of fitting a line or a curve, you are \"splitting\" the data based on conditions (e.g., is feature X greater than 5?). Each split creates a smaller group of data points, and you predict the target as the average value for that group.\n",
        "\n",
        "Example: \"If the house has more than 3 bedrooms, predict a higher price. Otherwise, predict a lower price.\"\n",
        "Key Insight: Decision trees capture non-linear relationships between the features and target by splitting the data into smaller regions, each of which has its own prediction.\n",
        "\n",
        "5. Random Forest Regression (Ensemble of Decision Trees)\n",
        "Goal: Combine multiple decision trees to improve prediction accuracy and reduce overfitting.\n",
        "\n",
        "Concept: Random forests build many decision trees and average their predictions to get a more robust result. This reduces the tendency of individual trees to overfit the data.\n",
        "\n",
        "Intuition: Imagine you have many decision trees, each making a prediction. Instead of trusting just one tree, you trust the majority vote (for classification) or the average (for regression) of many trees. This ensemble approach reduces overfitting and improves the generalization ability of the model.\n",
        "\n",
        "Key Insight: Ensemble methods like random forests leverage the wisdom of crowds: combining multiple weak models (trees) results in a stronger and more accurate overall model.\n",
        "\n",
        "6. Support Vector Regression (SVR)\n",
        "Goal: Fit a hyperplane while allowing some error (within a margin) to make predictions.\n",
        "\n",
        "Concept: SVR tries to fit a line (or hyperplane) that best represents the data while allowing some error within a defined margin. The key idea is to focus on support vectors (the points closest to the margin) and ignore the rest of the data.\n",
        "\n",
        "Intuition: You are fitting a line, but unlike linear regression, you're allowing some error if the data points fall within a margin of tolerance. Only the data points that are closest to this margin are important, and they determine the line or hyperplane.\n",
        "\n",
        "Key Insight: SVR focuses on the points that are most difficult to predict (close to the margin) and ignores points that are easily predictable (well inside the margin).\n",
        "\n",
        "7. K-Nearest Neighbors Regression (KNN)\n",
        "Goal: Predict the target based on the average of the\n",
        "ùêæ\n",
        "K closest points.\n",
        "\n",
        "Concept: KNN regression predicts the target for a new data point by averaging the target values of its\n",
        "ùêæ\n",
        "K closest neighbors in the feature space.\n",
        "\n",
        "Intuition: For a given data point, you look at the\n",
        "ùêæ\n",
        "K closest points from the training data (based on distance, like Euclidean distance). The prediction is simply the average of the target values of these neighbors.\n",
        "\n",
        "Key Insight: KNN regression makes predictions by relying on the assumption that similar points (based on features) should have similar targets."
      ],
      "metadata": {
        "id": "ozdV00oKbhGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "the next step after the regression the next is the classification\n",
        "Logistic Regression\n",
        "Goal: Predict the probability that a given input belongs to a certain class, usually for binary classification.\n",
        "\n",
        "Concept: Despite the name \"regression,\" logistic regression is used for classification. It models the probability of the target belonging to a class using a sigmoid function that outputs values between 0 and 1.\n",
        "\n",
        "Equation:\n",
        "\n",
        "  are the model's parameters (learned during training).\n",
        "Intuition: Logistic regression finds a linear decision boundary between two classes. It tries to find the best line (or hyperplane in higher dimensions) that separates the classes based on the features. The output is a probability, and the class is determined by a threshold (e.g., class 1 if probability > 0.5).\n",
        "\n",
        "2. K-Nearest Neighbors (KNN) Classifier\n",
        "Goal: Classify a new data point based on the majority class of its K nearest neighbors.\n",
        "Concept: KNN is a non-parametric method. It doesn‚Äôt learn an explicit model but instead relies on the entire dataset during prediction. For a new instance, KNN looks at the\n",
        "ùêæ\n",
        "K closest points and assigns the class based on the majority class of those neighbors.\n",
        "Intuition: If you're unsure of a new point‚Äôs class, you look at its nearest neighbors (based on some distance metric, e.g., Euclidean distance) and classify the point based on the class that is most common among those neighbors.\n",
        "Example: For\n",
        "ùêæ\n",
        "=\n",
        "3\n",
        "K=3, if 2 of the 3 nearest neighbors belong to class \"A\" and 1 belongs to class \"B,\" then the new point is classified as class \"A.\"\n",
        "3. Decision Tree Classifier\n",
        "Goal: Classify data by recursively partitioning the feature space into smaller regions based on feature values.\n",
        "Concept: Decision trees split the data at each step based on the most informative feature, creating a tree-like structure. At each node, the algorithm asks a question (e.g., \"Is feature X > 5?\"), and based on the answer, it moves to the next node or makes a classification decision.\n",
        "Intuition: Decision trees are simple to understand. They break down complex decision-making into a series of yes/no questions, which makes them very interpretable. The leaves of the tree represent the class labels, and the path from the root to the leaf is determined by feature values.\n",
        "Example: To classify a fruit as \"apple\" or \"orange,\" the tree might first ask, \"Is the fruit color red?\" and based on that answer, it continues asking other questions to decide the class.\n",
        "4. Random Forest Classifier\n",
        "Goal: Improve the accuracy and robustness of decision trees by combining multiple decision trees.\n",
        "Concept: Random forests are an ensemble method that builds many decision trees and combines their predictions. Each tree is trained on a random subset of the training data (bootstrap sampling) and features. The final prediction is made by taking the majority vote (for classification) or averaging the predictions (for regression) of all trees.\n",
        "Intuition: By aggregating many decision trees, random forests reduce the risk of overfitting that single decision trees often suffer from. Since each tree is trained on different data and features, the ensemble approach leads to more stable and accurate predictions.\n",
        "Example: If each tree classifies an instance as \"apple\" or \"orange,\" the class predicted by the majority of the trees is chosen.\n",
        "5. Support Vector Machine (SVM) Classifier\n",
        "Goal: Find the best hyperplane that separates different classes in the feature space.\n",
        "\n",
        "Concept: SVM works by finding a hyperplane that maximally separates the classes. The key idea is to find the hyperplane that has the largest margin (the distance between the nearest data points of each class, called support vectors).\n",
        "\n",
        "Intuition: Imagine you have two classes and want to separate them with a line (or plane in higher dimensions). SVM searches for the line (hyperplane) that maximizes the gap between the two classes while ensuring that no data points fall within this gap. This \"margin\" maximization helps the model generalize better.\n",
        "\n",
        "Example: For classifying a dog as \"cat\" or \"dog,\" SVM would find the best decision boundary that maximizes the distance between the closest instances of each class.\n",
        "Kernel Trick: For non-linear classification problems, SVM uses a technique called the kernel trick to map the input features into higher-dimensional space, where it becomes easier to find a linear separation.\n",
        "\n",
        "6. Naive Bayes Classifier\n",
        "Goal: Classify data based on Bayes‚Äô Theorem and the assumption of conditional independence between features.\n",
        "\n",
        "Concept: Naive Bayes is based on probability theory. It calculates the posterior probability of each class based on the likelihood of the input features, assuming the features are conditionally independent (which is why it‚Äôs called \"naive\").\n",
        "\n",
        "\n",
        "X.\n",
        "Intuition: Naive Bayes treats each feature as independent and calculates the class probability based on the product of the individual probabilities for each feature.\n",
        "\n",
        "Example: If you're classifying emails as \"spam\" or \"not spam,\" Naive Bayes computes the probability of an email being spam given the words in the email, assuming the words are independent of each other.\n",
        "7. Neural Networks (Multilayer Perceptrons)\n",
        "Goal: Learn complex patterns by passing data through layers of interconnected nodes (neurons).\n",
        "Concept: Neural networks consist of multiple layers of neurons, each of which performs a simple mathematical operation. The network learns the weights of the connections between neurons during training, adjusting them to minimize the classification error. Neural networks are highly flexible and can approximate complex functions.\n",
        "Intuition: Neural networks are inspired by the human brain. Each layer in the network transforms the data, and after many transformations, the network is able to classify data based on learned patterns. For classification, the final layer typically uses a softmax function to output probabilities for each class.\n",
        "Example: Neural networks are used for image classification (e.g., distinguishing between cats and dogs) or speech recognition.\n",
        "8. Gradient Boosting Machines (GBM)\n",
        "Goal: Build a strong classifier by combining many weak learners (typically decision trees).\n",
        "Concept: Gradient boosting is an ensemble method that builds trees sequentially. Each tree is trained to correct the errors (residuals) of the previous tree. The final prediction is the sum of the predictions from all the trees, with each tree trying to improve upon the last.\n",
        "Intuition: Instead of training all trees independently like in random forests, boosting trains each tree to improve upon the mistakes of the previous tree. It corrects the errors made by earlier trees, resulting in a strong, robust model.\n",
        "Example: GBM is widely used for classification tasks, like predicting whether a customer will churn based on their behavior.\n"
      ],
      "metadata": {
        "id": "tK6fKgppblw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "so after the regression and the classification the next step is the clustring the clustring is the unsupervised machine learning which have no label means the label in which that  not provide so after the no label means\n",
        "there are the some clustring tequenic are used in the ml likes are the\n",
        "k means clustring--> in the k means clustring we are set the some value of the cluster the model is starting with the random point and find the distance of each point form the centre and accoriding thsat we find the cluster so\n",
        "after the kmeans clustring .\n",
        "DBScan clustring--> in the density based spacial clustring\n",
        "in this clustring we are doing the actual so basically in this we are see the maximum density and acc to that we do clustring\n",
        "so in th DBSCAN we see the two impotant point one is the epsilon an the minpts\n",
        "the epsilon--> the maximum distance between the two point and the minpts--> the minimum points are requird to from the clustring\n",
        "1. K-Means Clustering\n",
        "Goal: Partition the data into\n",
        "ùêæ\n",
        "K clusters where each data point belongs to the cluster with the nearest mean.\n",
        "\n",
        "Concept: K-means works by selecting\n",
        "ùêæ\n",
        "K initial centroids (which can be randomly chosen) and assigning each data point to the nearest centroid. Then, the centroids are recalculated as the mean of all points in the cluster. This process continues iteratively until the centroids no longer change.\n",
        "\n",
        "Intuition: Imagine you're trying to group data points into\n",
        "ùêæ\n",
        "K distinct regions. K-means aims to minimize the sum of squared distances between the data points and the centroids of their respective clusters.\n",
        "\n",
        "Steps:\n",
        "Initialize\n",
        "ùêæ\n",
        "K centroids randomly.\n",
        "Assign each data point to the nearest centroid.\n",
        "Recalculate the centroids based on the points assigned to them.\n",
        "Repeat steps 2 and 3 until convergence.\n",
        "Key Insight: K-means is sensitive to the initial placement of centroids and might converge to a local minimum. It also assumes that clusters are spherical and of equal size.\n",
        "\n",
        "2. Hierarchical Clustering\n",
        "Goal: Create a hierarchy of clusters, which can be visualized as a tree-like structure (dendrogram).\n",
        "\n",
        "Concept: Hierarchical clustering builds a tree of clusters in a bottom-up (agglomerative) or top-down (divisive) manner. It doesn't require the number of clusters\n",
        "ùêæ\n",
        "K to be specified in advance. Instead, the algorithm builds a hierarchy of clusters and then cuts the tree at the desired level to get the final clusters.\n",
        "\n",
        "Agglomerative (bottom-up): Starts with each data point as its own cluster and iteratively merges the closest clusters until only one cluster remains.\n",
        "Divisive (top-down): Starts with all data points in one large cluster and splits it iteratively.\n",
        "Intuition: Imagine you have a group of objects. In agglomerative hierarchical clustering, you start by treating each object as its own group, then merge the closest objects together, building a hierarchy of clusters. The closer objects get merged first, and as the hierarchy grows, you get a more comprehensive view of how data points are related.\n",
        "\n",
        "Key Insight: Hierarchical clustering provides a tree structure (dendrogram), which gives insight into how clusters are formed. It doesn‚Äôt require the number of clusters to be specified upfront but can be computationally expensive.\n",
        "\n",
        "3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
        "Goal: Identify clusters based on areas of high density of data points.\n",
        "\n",
        "Concept: DBSCAN is a density-based clustering algorithm. It defines clusters as areas with a high density of points, separated by areas with low point density. It requires two parameters:\n",
        "\n",
        "Epsilon (Œµ): The maximum distance between two points to be considered neighbors.\n",
        "MinPts: The minimum number of points required to form a dense region (i.e., a cluster).\n",
        "DBSCAN works by labeling points as:\n",
        "\n",
        "Core points: Points that have more than MinPts points within the Œµ radius.\n",
        "Border points: Points that are within the Œµ radius of a core point but have fewer than MinPts neighbors.\n",
        "Noise points: Points that do not belong to any cluster.\n",
        "Intuition: Imagine you‚Äôre looking for groups of points that are packed closely together, surrounded by space with few points. DBSCAN looks for dense regions of data and forms clusters accordingly. Points that are alone in sparse regions are marked as noise.\n",
        "\n",
        "Key Insight: DBSCAN is non-parametric (doesn‚Äôt require you to specify the number of clusters). It can find clusters of arbitrary shapes and is robust to noise, but it may struggle with clusters of varying densities.\n",
        "\n",
        "4. Gaussian Mixture Model (GMM)\n",
        "Goal: Model the data as a mixture of several Gaussian distributions, each representing a cluster.\n",
        "\n",
        "Concept: GMM assumes that data points are generated from a combination of several Gaussian distributions. Each cluster is modeled as a Gaussian (normal) distribution, and the algorithm tries to fit these Gaussian distributions to the data. The model assigns each data point a probability of belonging to each cluster, unlike K-means, which assigns points to only one cluster.\n",
        "\n",
        "Intuition: GMM tries to find a probabilistic clustering of the data. Instead of hard assignments, each point has a certain probability of belonging to each cluster, making GMM more flexible than K-means for data that may not be perfectly spherical.\n",
        "\n",
        "Key Insight: GMM is more flexible than K-means because it can handle elliptical clusters and probabilistic assignments, but it can be computationally more expensive.\n",
        "\n",
        "5. Mean Shift Clustering\n",
        "Goal: Find areas of high data point density and group data points around these areas.\n",
        "\n",
        "Concept: Mean shift is a non-parametric clustering algorithm that doesn't require the number of clusters to be specified. It works by iteratively shifting data points towards the mean of the points in a neighborhood (i.e., finding the mode of the data distribution). Points that converge to the same mean are grouped together.\n",
        "\n",
        "Intuition: Imagine you have a set of data points. Mean shift starts by calculating the \"center of mass\" of nearby points and moves each point towards this center. Over time, points that are close to each other will shift toward the same region, forming clusters.\n",
        "\n",
        "Key Insight: Mean shift can find clusters of arbitrary shapes and doesn‚Äôt require the number of clusters to be pre-specified. However, it can be computationally expensive, especially for large datasets.\n",
        "\n",
        "6. Affinity Propagation\n",
        "Goal: Find exemplars (representative data points) and assign other points to the closest exemplar.\n",
        "\n",
        "Concept: Affinity propagation works by identifying a set of exemplars (data points that serve as the center of clusters) and then assigning every other point to the closest exemplar. It uses a similarity matrix and a message-passing algorithm to determine which points are exemplars and which belong to them.\n",
        "\n",
        "Intuition: Instead of specifying the number of clusters, affinity propagation uses similarity between points to determine how many clusters are appropriate. The algorithm passes messages between points, and over time, some points are chosen as \"exemplars\" (central points for clusters).\n",
        "\n",
        "Key Insight: Affinity propagation is able to automatically determine the number of clusters based on the data, making it a good choice when the number of clusters is unknown. However, it can be computationally expensive.\n",
        "\n",
        "7. Spectral Clustering\n",
        "Goal: Use the eigenvalues of a similarity matrix to reduce dimensionality and then apply a standard clustering algorithm.\n",
        "\n",
        "Concept: Spectral clustering uses a similarity matrix (which defines how similar points are to each other) to construct a graph where points are connected if they are similar. The algorithm then uses the Laplacian matrix to compute the eigenvalues and eigenvectors, which are used to reduce the dimensionality of the problem. A standard clustering algorithm like K-means is applied to the reduced data.\n",
        "\n",
        "Intuition: Imagine you have a graph where points are nodes and edges represent similarities. Spectral clustering uses the structure of this graph to find the best clusters. By focusing on the graph's eigenvalues and eigenvectors, spectral clustering can detect non-convex clusters that traditional methods like K-means may miss.\n",
        "\n",
        "Key Insight: Spectral clustering is very useful for complex, non-linearly separable clusters. It works well for detecting clusters in graphs or in cases where the data structure is not well-defined.\n",
        "\n",
        "8. Self-Organizing Maps (SOM)\n",
        "Goal: Create a low-dimensional (usually 2D) representation of high-dimensional data.\n",
        "\n",
        "Concept: Self-organizing maps are neural network-based algorithms that project high-dimensional data onto a 2D grid of neurons. These neurons self-organize over time to represent the data, and similar data points are mapped to neighboring neurons.\n",
        "\n",
        "Intuition: SOM takes high-dimensional data and maps it to a 2D grid where similar points end up near each other. It's often used for visualizing complex data structures or for reducing the dimensionality of data.\n",
        "\n",
        "Key Insight: SOM is a good method for visualizing high-dimensional data, but it‚Äôs not as widely used for general clustering tasks compared to other methods like K-means or DBSCAN"
      ],
      "metadata": {
        "id": "8mrf2PAed9Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of Classification Models\n",
        "classifiers = [\n",
        "    (\"Logistic Regression\", LogisticRegression(max_iter=200)),\n",
        "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
        "    (\"Random Forest\", RandomForestClassifier()),\n",
        "    (\"Gradient Boosting\", GradientBoostingClassifier()),\n",
        "    (\"SVM\", SVC()),\n",
        "    (\"KNN\", KNeighborsClassifier()),\n",
        "    (\"Naive Bayes\", GaussianNB())\n",
        "]\n",
        "\n",
        "# Training and Evaluating Classification Models\n",
        "print(\"Classification Results:\")\n",
        "for name, model in classifiers:\n",
        "    model.fit(X_train_class_scaled, y_train_class)\n",
        "    y_pred_class = model.predict(X_test_class_scaled)\n",
        "    accuracy = accuracy_score(y_test_class, y_pred_class)\n",
        "    print(f\"{name}: Accuracy = {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Vsq0Bo_9mH-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Classification Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n"
      ],
      "metadata": {
        "id": "FhRw4rHvmg_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of Classification Models\n",
        "classifiers = [\n",
        "    (\"Logistic Regression\", LogisticRegression(max_iter=200)),\n",
        "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
        "    (\"Random Forest\", RandomForestClassifier()),\n",
        "    (\"Gradient Boosting\", GradientBoostingClassifier()),\n",
        "    (\"SVM\", SVC()),\n",
        "    (\"KNN\", KNeighborsClassifier()),\n",
        "    (\"Naive Bayes\", GaussianNB()),\n",
        "    (\"MLP Classifier\", MLPClassifier(max_iter=1000)),\n",
        "    (\"Perceptron\", Perceptron()),\n",
        "    (\"Quadratic Discriminant Analysis\", QuadraticDiscriminantAnalysis()),\n",
        "    (\"Gaussian Process Classifier\", GaussianProcessClassifier()),\n",
        "    (\"Calibrated Classifier\", CalibratedClassifierCV(LogisticRegression()))\n",
        "]\n",
        "\n",
        "# Train and evaluate the models\n",
        "print(\"Classification Results:\")\n",
        "for name, model in classifiers:\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{name}: Accuracy = {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "sR2MbFxQmtdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "now after taking the high amount of the fuck in the clustring and after the stuck in the clustring the next step is the apply the go deep in the clustring i have to lean  the clustring"
      ],
      "metadata": {
        "id": "Cx98wtW0vGWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "so what is the code\n",
        "first import the clustring teqnic\n",
        "from sklearn.cluster import kmeans\n",
        "cluster=kmeans(n_clusters=4)\n",
        "cluster.fit(X_reshaped)\n",
        "label=cluster.labels\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(range(len(X)), X, c=labels, cmap='viridis', s=100)\n",
        "\n",
        "# Show the cluster centroids\n",
        "centroids = cluster.cluster_centers_\n",
        "plt.scatter(range(len(X)), centroids[labels], c='red', marker='X', s=200, label='Centroids')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title(\"K-Means Clustering with Categorical Data\")\n",
        "plt.xlabel(\"Index\")\n",
        "plt.ylabel(\"Category\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fLiIuI8wxmFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "so after the understanding of the clustering the next step is the association rule learning the motimportnt fundai in the ecommerce  site"
      ],
      "metadata": {
        "id": "cEWEF2hiyh5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "so what is the association rule learning so the main factor in the association rule learning is that the  the association rule learning is mostly used in the market basket anlaysis is the depend on the threee factor\n",
        "1 . support-->what is support -->the suppot is nothing but the"
      ],
      "metadata": {
        "id": "jha0_PrzywdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A frequent itemset is a set of items that appear together in transactions with frequency greater than a specified threshold, called support.\n"
      ],
      "metadata": {
        "id": "nfscx2ZQ3bFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Support:\n",
        "\n",
        "Support refers to how frequently an itemset appears in the dataset.\n",
        "Mathematically:\n",
        "Support\n",
        "(\n",
        "ùê¥\n",
        ")\n",
        "=\n",
        "Transactions¬†containing\n",
        "ùê¥\n",
        "Total¬†number¬†of¬†transactions\n",
        "Support(A)=\n",
        "Total¬†number¬†of¬†transactions\n",
        "Transactions¬†containing¬†A\n",
        "‚Äã\n",
        "\n",
        "This tells us how often an item or itemset appears in the dataset.\n"
      ],
      "metadata": {
        "id": "ExOGplb_3cGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Association Rule Learning:\n",
        "Association rule learning is a machine learning technique used to discover interesting relationships or patterns between variables in large datasets, particularly for transactional data. It is most commonly applied in market basket analysis, where the goal is to find associations between products bought together by customers.\n",
        "\n",
        "The goal of association rule learning is to extract frequent itemsets (sets of items that frequently co-occur) and derive association rules from them. These rules describe the relationships between items in the dataset, and they are typically used for making recommendations, finding correlations, or optimizing processes.\n",
        "\n",
        "Key Concepts of Association Rule Learning:\n",
        "Association Rule:\n",
        "\n",
        "\n",
        "This means that if A occurs (e.g., a customer buys product A), then B is likely to occur (e.g., the same customer is likely to buy product B). The rule is interpreted as \"if A, then B.\"\n",
        "The rule is not deterministic but probabilistic‚Äîmeaning it's likely, not guaranteed, that B will occur if A occurs.\n",
        "Itemsets:\n",
        "\n",
        "An itemset is simply a collection of one or more items from the dataset.\n",
        "A frequent itemset is a set of items that appear together in transactions with frequency greater than a specified threshold, called support.\n",
        "Support:\n",
        "\n",
        "Support refers to how frequently an itemset appears in the dataset.\n",
        "M\n",
        "This tells us how often an item or itemset appears in the dataset.\n",
        "Confidence:\n",
        "\n",
        "Confidence is a measure of the reliability of the inference made by the rule. It represents the likelihood of finding item B in a transaction when item A is already present.\n",
        "Mathematically:\n",
        "This tells us how likely item B will be bought if item A is bought.\n",
        "Lift:\n",
        "\n",
        "Lift is a metric that evaluates the strength of an association rule relative to the expected occurrence of item B. A lift value greater than 1 suggests that the items are likely to be bought together more often than expected by chance.\n",
        "Mathematically:\n",
        "L\n",
        "Lift tells you how much more likely B is to occur when A occurs compared to when B would occur randomly.\n",
        "Process of Association Rule Learning:\n"
      ],
      "metadata": {
        "id": "daL_BU_R3hf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "import pandas as pd\n",
        "\n",
        "# Sample transaction data (boolean format: 1 = item purchased, 0 = not purchased)\n",
        "data = pd.DataFrame({\n",
        "    'Bread': [1, 1, 1, 1, 0],\n",
        "    'Milk': [1, 1, 0, 1, 1],\n",
        "    'Diaper': [0, 1, 1, 1, 0],\n",
        "    'Beer': [0, 1, 1, 1, 0],\n",
        "    'Eggs': [1, 1, 0, 0, 1],\n",
        "})\n",
        "\n",
        "# Step 1: Find frequent itemsets using the Apriori algorithm\n",
        "frequent_itemsets = apriori(data, min_support=0.6, use_colnames=True)\n",
        "\n",
        "# Step 2: Generate association rules from frequent itemsets\n",
        "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
        "\n",
        "# Step 3: Display the generated rules\n",
        "print(rules)"
      ],
      "metadata": {
        "id": "XsZTaKj44C5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "so what is next in the feild of the machine learning is the nlp the\n",
        "what are the basic step in the nlp\n",
        "the fisrt step in the nlp is the data equnization\n",
        "so first we take the data from the our data e"
      ],
      "metadata": {
        "id": "KPBED5gT4blD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}