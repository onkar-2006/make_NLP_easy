{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tar5kluIAYlf"
      },
      "outputs": [],
      "source": [
        "#lets start the game of the begin so lets start\n",
        "data=pd.read_csv(\"data.csv\")\n",
        "X=data.iloc[:,-1].values\n",
        "y=data.iloc[:,:-1].values\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#after loading the data in the file the next step is the starting the deep learning\n",
        "so what is the most important in the deep learning that ar the some term\n",
        "1.artifical nural network\n",
        "2.convolutional nural network\n",
        "3.recurrent nural network\n",
        "4.ganerative nuralnetwork\n",
        "5.autoencoder\n",
        "6.som(self orgenzing map)\n",
        "7.transformer\n",
        "8.boltzman machine\n",
        "\n",
        "now our aim is to finish all the 8 nural network  in one one"
      ],
      "metadata": {
        "id": "fQKnYl9QAs36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-->lets start with single layer perseptron nural network\n",
        "-->there are the some imp --> start with the -->ann-->the imp point are--> what is the node, nuron , bias ,\n"
      ],
      "metadata": {
        "id": "zQFDdxTIBiod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-->what are the imp--> the nural network is make form the three basic things are\n",
        "--> input_layers-->hidden_layers-->output\n",
        "-->input_layers--> the data we given to the model is taking from the input layers\n",
        "-->hidden_layer-->the hidden layer is used to increse the complexity of the model--> by nicreseing the hidden layers--> we can capture te hiddenpattern in the data\n",
        "-->output_layer-->the output is taken form the output layers"
      ],
      "metadata": {
        "id": "WX2gAwevCNiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "--> the some basic consept in the neural network\n",
        "-->Neourn--> the neuron recive input and process the the data--> produce the output\n",
        "-->weight --> the weight detemine the strength of the connection between the two nueroneach input to the neuron is multiply by weight\n",
        "-->the bais is the hyper parameter help to shift the activation function\n",
        "-->activation_function--> the activation function determine whether  the nuron should be activted or not\n",
        "--> the activation fuction is used to capture the non-liniearity in the data\n",
        "-->it is the simple mathamatical function --> means basically the activation function convert that higher values into the which taken the input form the hidden layer and convert that\n",
        "input into the small value\n",
        "-->loss-->the loss function or the cost function it is the function which is the diffrence betwenn the actual value and  the predicted values"
      ],
      "metadata": {
        "id": "kJYUvPsPC1JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-->what is the forword process\n",
        "what is feed inside the activation function--> the weighted sum-->the weighted sum is the multiplication of the input fetrue and the wgith with\n",
        "the bais the used of the bais is that for the better performance and for shifting the activation function\n",
        "-->the string of the neural network --> feed forword network-->\n",
        "input layer-->weighted_sum-->activation_layer-->hidden layer-->output-->output-prediction\n",
        "\n",
        "input_layers-->the input is feed into in the input layer\n",
        "wieghted_sum--> the weighted sum is multiplication of the weight and the input values\n",
        "weighted_sum-->w1*x1+w2*x2...+wn*xn+b\n",
        "activation_function-->the weighted_sum(w1*x1+w2*x2...+wn*xn+b) is feed into the activation the activation fuction are the mathamatical function-->\n",
        "the mathamtical function that convert the wieght sum in the small range like the (-1 to 1) there are the somre activation function are\n",
        "--relu(0 ,max) , selu , liklyrelu , softmax(give the probabily (0,1)), tanh(-1, 1),"
      ],
      "metadata": {
        "id": "DPEeUYZOFP_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "so the next the activation apply the next is feed that value into the next heddin layer repeted each to the result is feed into the next hidden layer\n",
        "the after the the each hidden layer is taken the input from the previous output of the layer and feed as the input of the next hidden layer\n",
        "the output layer made the predition\n",
        "the output layer provide the final reusult of the feedforword layer after the forword propogetion the backword process is start\n",
        "the backpropogetion is the algorithim used for training the neural network\n",
        "it allow the network the learn bu adjecting the weight in order to minimize the loss between the predicted and the actual\n",
        " It works by computing the gradient of the loss function with respect to each weight by applying the chain rule of calculus in reverse through the networ"
      ],
      "metadata": {
        "id": "jjiMXMB-H6T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "the main aim of the nural network to minimize the error the when we can said the model is working good --> when the error is 0 means the model has the 100% accuracy\n",
        "Gradient of the Loss Function: The backpropagation process calculates the gradient of the loss function with respect to each weight in the network.\n",
        "The gradient tells us how to adjust the weights to reduce the error. Itâ€™s essentially the derivative of the loss with respect to the weights.\n",
        "Calculating the gradient of the error (loss) with respect to each weight using the chain rule of calculus.\n",
        "Propagating this error backward through the network, starting from the output layer and moving towards the input layer.\n",
        "Once the gradients are computed, we need to update the weights to reduce the error\n",
        "This is typically done using Gradient Descent or a variant like Stochastic Gradient Descent (SGD), where weights are updated by subtracting a portion of the gradient from the current weight. The size of the step is controlled by a parameter called the learning rate."
      ],
      "metadata": {
        "id": "D58-mhXDKN_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "what is the perseptron is the simple regression by case lets start the sppose we have the classification take so in that we are actually the next step is the whe it ask to the each point of the data\n",
        "whether that point is rightly classifes if yes then the line do not move if not then the line moves toword that point"
      ],
      "metadata": {
        "id": "k8cVW0kqL-pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "so in the multilayer perseptron we combine the single layer perseptron combine the single layer and combine the single layer basically it help the increse the non linearity\n",
        "the non liniearity is capture the single layer\n"
      ],
      "metadata": {
        "id": "QMmAQiaHOig5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "typs of the activation function and the types of loss function\n",
        "the type of the activation function\n",
        "Sigmoid: Binary classification, probability outputs.\n",
        "Tanh: Hidden layers, recurrent networks (when negative values are important).\n",
        "ReLU: Most hidden layers in deep networks, convolutional layers, and generative models.\n",
        "Leaky ReLU: Deep networks where ReLU might cause dead neurons.\n",
        "PReLU: Deep networks where the slope of negative values should be learned.\n",
        "ELU: Deep networks requiring faster convergence and smooth activation.\n",
        "Softmax: Multi-class classification in the output layer.\n",
        "Swish: Complex networks requiring improved performance (e.g., image and speech recognition).\n",
        "Softplus: Alternative to ReLU for smoother outputs.\n",
        "Mish: Cutting-edge models requiring robust learning and performance improvements.\n",
        "\n"
      ],
      "metadata": {
        "id": "S0m0UPCsPFxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses\n",
        "Regression: MSE, MAE, Huber Loss, Poisson Loss\n",
        "Binary Classification: Binary Cross-Entropy\n",
        "Multi-Class Classification: Categorical Cross-Entropy, Sparse Categorical Cross-Entropy\n",
        "Similarity Learning: Triplet Loss, Cosine Similarity Loss\n",
        "Support Vector Machines: Hinge Loss\n",
        "Probabilistic Models: Kullback-Leibler Divergence\n",
        "Imbalanced Classification: Focal Loss"
      ],
      "metadata": {
        "id": "Rkwy7J-cSGzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type of the gredint desent\n",
        "BGD: Uses the entire dataset, slow but precise.\n",
        "SGD: Uses one data point at a time, faster but noisy.\n",
        "Mini-batch GD: Uses small batches, balances between speed and accuracy.\n",
        "Momentum/Nesterov: Accelerates convergence by considering past gradients.\n",
        "Adagrad/RMSprop: Adaptive learning rates for each parameter.\n",
        "Adam/Nadam: Adaptive learning rates with momentum, highly effective in most case"
      ],
      "metadata": {
        "id": "SrKQgJtiTcwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type of the optimizer\n",
        "SGD: Simple and traditional, works well for large datasets.\n",
        "Momentum: Helps accelerate convergence and smooth out oscillations.\n",
        "NAG: Improves momentum with lookahead for better accuracy.\n",
        "Adagrad: Adaptive learning rates for sparse data.\n",
        "RMSprop: An improvement on Adagrad, works well for non-stationary problems.\n",
        "Adam: Combines momentum and adaptive learning rates, widely used and effective.\n",
        "Nadam: Combines Adam and Nesterov momentum for faster convergence.\n",
        "Adadelta: Like Adagrad but adjusts learning rates to prevent rapid decay.\n",
        "FTRL: Used in online learning and large-scale sparse data problems.\n",
        "LBFGS: A second-order optimization method, used for smaller datasets."
      ],
      "metadata": {
        "id": "yK2lbZqZWiq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "the  lets stsr t bulild ing the nural netwerk after all the basic it is very simpleto build the nural network\n"
      ],
      "metadata": {
        "id": "iTK8ZpVVUp32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers , Dense , models\n",
        "model=tf.keras.models.Sequentail()\n",
        "model.add(tf.keras.layers.add(units=32, activation=\"relu\" , input_shape=(12 ,)))\n",
        "model.add(tf.keras.layers.add(uints=128 , activation=\"selu\"))\n",
        "model.add(tf.keras.layers.add(uints=128 , activation=\"selu\"))\n",
        "model.add(tf.keras.layers.add(uints=1 , activation=\"sigmoid\"))\n",
        "\n",
        "model.compile( optimizer=\"adam\", loss=\"binary_cross_entropy\" , metric=[\"accuracy\"])\n",
        "model.fit(batch_size=32 , x_train , y_train , epochs=100 , verbose=1 )"
      ],
      "metadata": {
        "id": "dl1HMX-OVXj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Convolution neural network\n",
        "the basic of the conv\n",
        "--> the representation of the image as we know rhat every image is representation of the pixel and each pixel have its intensity value so the intensity i\n",
        "con2D layer\n",
        "maxpool layer\n",
        "flatten layer\n",
        "fully connected layer\n",
        "stride\n",
        "padding\n",
        "\n"
      ],
      "metadata": {
        "id": "oFvETkF7WlqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WTSUeoDXXgki"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}