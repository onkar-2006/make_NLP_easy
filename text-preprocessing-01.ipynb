{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30152,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"df = pd.read_csv('load your own dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0,len(df)):\n    new_df = df['review'][i].lower()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review'].str.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review'] = df['review'].str.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef remove_html_tags(text):\n    pattern = re.compile('<.*?>')\n    return pattern.sub(r'', text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remove_html_tags(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review'] = df['review'].apply(remove_html_tags)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_url(text):\n    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return pattern.sub(r'', text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nstring.punctuation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exclude = string.punctuation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_punc(text):\n    for char in exclude:\n        text = text.replace(char,'')\n    return text\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time1/time2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chat_conversion(text):\n    new_text = []\n    for w in text.split():\n        if w.upper() in chat_words:\n            new_text.append(chat_words[w.upper()])\n        else:\n            new_text.append(w)\n    return \" \".join(new_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textblob import TextBlob","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ntextBlb = TextBlob(\"write ur dataset name here\")\n\ntextBlb.correct().string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_stopwords(text):\n    new_text = []\n    \n    for word in text.split():\n        if word in stopwords.words('english'):\n            new_text.append('')\n        else:\n            new_text.append(word)\n    x = new_text[:]\n    new_text.clear()\n    return \" \".join(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review'].apply(remove_stopwords)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import emoji\nprint(emoji.demojize(\"your dataset name\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Using the split function","metadata":{}},{"cell_type":"markdown","source":"### 2. Regular Expression","metadata":{}},{"cell_type":"code","source":"import re\nsent3 = 'I am going to delhi!'\ntokens = re.findall(\"[\\w']+\", sent3)\ntokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsentences = re.compile('[.!?] ').split(\"write ur dataset name here\")\nsentences","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. NLTK","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize,sent_tokenize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nword_tokenize(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \nLorem Ipsum has been the industry's standard dummy text ever since the 1500s, \nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n\nsent_tokenize(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent5 = 'I have a Ph.D in A.I'\nsent6 = \"We're here to help! mail us at nks@gmail.com\"\nsent7 = 'A 5km ride cost $10.50'\n\nword_tokenize(sent5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Spacy","metadata":{}},{"cell_type":"code","source":"import spacy\nnlp = spacy.load('en_core_web_sm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc1 = nlp(sent5)\ndoc2 = nlp(sent6)\ndoc3 = nlp(sent7)\ndoc4 = nlp(sent1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for token in doc4:\n    print(token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ps = PorterStemmer()\ndef stem_words(text):\n    return \" \".join([ps.stem(word) for word in text.split()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nstem_words(\"write ur dataset name here\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\n\npunctuations=\"?:!.,;\"\nsentence_words = nltk.word_tokenize(\"write here ur dataset name\")\nfor word in sentence_words:\n    if word in punctuations:\n        sentence_words.remove(word)\n\nsentence_words\nprint(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\nfor word in sentence_words:\n    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}